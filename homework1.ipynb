{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# this project is finished by Tianlin Zhang(A20345584) and Li ma(A20346352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_recall_curve  \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV,Ridge\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 0. preparation \n",
    "before using the different training model, we should deal with the data first, in this part we will create the trainning set of clean and full data, as well as the label for regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " a. let's deal with the clean data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read the clean data using pandas\n",
    "df = pd.read_csv(\"communities-crime-clean.csv\")\n",
    "#define a function for classification output label \n",
    "def highcrime (row):\n",
    "    if (row['ViolentCrimesPerPop'] > 0.1):\n",
    "        return 1\n",
    "    return 0\n",
    "tmp = df.apply(lambda row:highcrime(row),axis = 1)\n",
    "#we get the y for classification and y for regression\n",
    "clean_clf_y = tmp.values.tolist()\n",
    "clean_reg_y = df['ViolentCrimesPerPop']\n",
    "#now del the non predictive feature\n",
    "del df[\"state\"]\n",
    "del df[\"communityname\"]\n",
    "del df[\"fold\"]\n",
    "del df['ViolentCrimesPerPop']\n",
    "# now we get the clean training set x\n",
    "clean_X = df\n",
    "#and the name of all the features\n",
    "clean_features = list(df.columns[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "b. now deal with the full data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read the full data using pandas\n",
    "df2 = pd.read_csv(\"communities-crime-full.csv\")\n",
    "#deal with the '?' and unralated feature\n",
    "df2 = df2.replace('?', np.nan)\n",
    "del df2[\"state\"]\n",
    "del df2[\"county\"]\n",
    "del df2[\"community\"]\n",
    "del df2[\"communityname\"]\n",
    "del df2[\"fold\"]\n",
    "#we get the y for regression\n",
    "full_reg_y = df2[\"ViolentCrimesPerPop\"]\n",
    "del df2[\"ViolentCrimesPerPop\"]\n",
    "#deal with it to get the clf y\n",
    "full_clf_y = full_reg_y.copy()\n",
    "for index, item in enumerate(full_clf_y):\n",
    "    if (item > 0.1):\n",
    "        full_clf_y[index] = 1\n",
    "    else:\n",
    "        full_clf_y[index] = 0\n",
    "#we deal the x now and fill the missing data using mean of the exist data\n",
    "imp = Imputer(missing_values= \"NaN\", strategy='mean', axis=0)\n",
    "full_X = imp.fit_transform(df2)\n",
    "#and all the features\n",
    "full_features = list(df2.columns[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "now we can do the project now\n",
    "### 1. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### a. get the crime rate and calculate the percentage of positive and negative example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the positve percentage is:\n",
      "0.6271951831409934\n",
      "the negative percentage is:\n",
      "0.3728048168590065\n"
     ]
    }
   ],
   "source": [
    "#the highCrime is the same as clean_clf_y\n",
    "highCrime = clean_clf_y\n",
    "#the positive number and neg number is \n",
    "pos = highCrime.count(1)\n",
    "neg = highCrime.count(0)\n",
    "#the percentage\n",
    "print(\"the positve percentage is:\")\n",
    "print(pos/len(highCrime))\n",
    "print(\"the negative percentage is:\")\n",
    "print(neg/len(highCrime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### b.  Use DecisionTreeClassifier to learn a decision tree to predict highCrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#init the DT clf\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "#fit the data\n",
    "dt_clf.fit(clean_X,clean_clf_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " i. the accuracy, precision and recall are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy is  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       1.00      1.00      1.00       743\n",
      "   positive       1.00      1.00      1.00      1250\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = dt_clf.predict_proba(clean_X)[:,1]  \n",
    "accuracy = accuracy_score(clean_clf_y, dt_clf.predict(clean_X))  \n",
    "print(\"the accuracy is \",accuracy)\n",
    "print(classification_report(clean_clf_y, answer, target_names = ['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii. now let's find the main features according to the gini importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PctKids2Par 0.362097595066\n",
      "racePctWhite 0.0888168559837\n",
      "racePctHisp 0.0485853556595\n",
      "PctLess9thGrade 0.0235240057788\n",
      "PctEmplManu 0.0181445204434\n",
      "MedRent 0.0164670739976\n",
      "PctWOFullPlumb 0.01450624349\n",
      "PctVacMore6Mos 0.0128805049573\n",
      "PctEmploy 0.0121760645547\n",
      "PctRecImmig5 0.0115733067422\n",
      "PctSameState85 0.0115333819062\n"
     ]
    }
   ],
   "source": [
    "main_feature = dict(zip(clean_features,dt_clf.feature_importances_))\n",
    "for index,w in enumerate(sorted(main_feature, key=main_feature.get, reverse=True)):\n",
    "    print(w ,main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above result is make sense, because the decision tree is create by get the minimum entropy at every decision, and from the data we can find that if we order the data with the main features above, they are all high related to the crime rate. So the above result make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### c. Now apply cross-validation (cross_val_score) to do 10-fold cross-validation to estimate the out-of-training accuracy of decision tree learning for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i.\tWhat are the 10-fold cross-validation accuracy, precision, and recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10-fold cv is:\n",
      "[ 0.725       0.815       0.78        0.76884422  0.69849246  0.65829146\n",
      "  0.74371859  0.69346734  0.76884422  0.71356784]\n",
      "the precision for 10-fold cv is:\n",
      "[ 0.75        0.81203008  0.77692308  0.76865672  0.76530612  0.76923077\n",
      "  0.75182482  0.83157895  0.76870748  0.80530973]\n",
      "the recall for 10-fold cv is:\n",
      "[ 0.856  0.88   0.824  0.832  0.608  0.656  0.832  0.672  0.88   0.744]\n",
      "the avg accuracy is: 0.736522613065\n",
      "the avg precision is: 0.77995677426\n",
      "the avg recall is: 0.7784\n"
     ]
    }
   ],
   "source": [
    "ten_fold_accuracy = cross_val_score(dt_clf, clean_X, clean_clf_y, cv=10,scoring='accuracy')\n",
    "ten_fold_precision = cross_val_score(dt_clf, clean_X, clean_clf_y, cv=10,scoring='precision')\n",
    "ten_fold_recall = cross_val_score(dt_clf, clean_X, clean_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10-fold cv is:\")\n",
    "print(ten_fold_accuracy)\n",
    "print(\"the precision for 10-fold cv is:\")\n",
    "print(ten_fold_precision)\n",
    "print(\"the recall for 10-fold cv is:\")\n",
    "print(ten_fold_recall)\n",
    "print(\"the avg accuracy is:\",sum(ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii.Why are they different from the results in the previous test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "there are several reasons to cause this result\n",
    "        1. we do not doing the feature reduction or layer limit for the decision tree, so when we doing the dt training for the all data, we have the overfit which make all the result is 100%.\n",
    "        2. for the 10-fold part, because we doing the corss validation, so it will decrease the rate of overfit, which we also decrease the accuracy, precasion, recall for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### a. Use GaussianNB to learn a Naive Bayes classifier to predict highCrime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i. What is the 10-fold cross-validation accuracy, precision, and recall for this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10 cv is:\n",
      "[ 0.775       0.8         0.825       0.79899497  0.70351759  0.65326633\n",
      "  0.81407035  0.73366834  0.71356784  0.79899497]\n",
      "the precision for 10 cv is:\n",
      "[ 0.86363636  0.92929293  0.95        0.92079208  0.94594595  0.86842105\n",
      "  0.92307692  1.          0.77868852  0.93814433]\n",
      "the recall for 10 cv is:\n",
      "[ 0.76   0.736  0.76   0.744  0.56   0.528  0.768  0.576  0.76   0.728]\n",
      "the avg accuracy is: 0.761608040201\n",
      "the avg precision is: 0.911799814828\n",
      "the avg recall is: 0.692\n"
     ]
    }
   ],
   "source": [
    "#init the gnb clf\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(clean_X,clean_clf_y)\n",
    "nb_clf_accuracy = cross_val_score(nb_clf, clean_X, clean_clf_y, cv=10,scoring='accuracy')\n",
    "nb_clf_precision = cross_val_score(nb_clf, clean_X, clean_clf_y, cv=10,scoring='precision')\n",
    "nb_clf_recall = cross_val_score(nb_clf, clean_X, clean_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10 cv is:\")\n",
    "print(nb_clf_accuracy)\n",
    "print(\"the precision for 10 cv is:\")\n",
    "print(nb_clf_precision)\n",
    "print(\"the recall for 10 cv is:\")\n",
    "print(nb_clf_recall)\n",
    "print(\"the avg accuracy is:\",sum(nb_clf_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(nb_clf_precision)/10)\n",
    "print(\"the avg recall is:\",sum(nb_clf_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii.\tWhat are the 10 most predictive features? Why do these make sense (or not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PctKids2Par 5.00131022903\n",
      "FemalePctDiv 4.75688596283\n",
      "PctFam2Par 4.59398800551\n",
      "pctWInvInc 4.39499084626\n",
      "TotalPctDiv 4.38598607605\n",
      "PctTeen2Par 3.99935365439\n",
      "MalePctDivorce 3.95073141349\n",
      "PctYoungKids2Par 3.64830107618\n",
      "PctIlleg 3.4698885122\n",
      "racePctWhite 3.44322831319\n",
      "PctHousLess3BR 3.29878847562\n"
     ]
    }
   ],
   "source": [
    "nb_main_feature = dict(zip(clean_features,abs(nb_clf.theta_[0] - nb_clf.theta_[1])/(nb_clf.sigma_[0] + nb_clf.sigma_[1])))\n",
    "for index,w in enumerate(sorted(nb_main_feature, key=nb_main_feature.get, reverse=True)):\n",
    "    print(w ,nb_main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the result of the main feature is make sense, because according to the form we are using, what we trying to find is the the feature that have the largest class mean difference with low variance. When the result of the (ut - uf)/(sigmat + sigmaf) is larger, it means that they have large mean difference or have smaller variance or both. Which means they have a relatively large gap when using that feature to do the classification. So the main feature is make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "iii. How do these results compare with your results from decision trees above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "most main features is similiar with the decision tree, with a slight difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the 10-fold cv result we can find that the accuracy and precision using NB is better than using the decision tree, while the recall of NB is worse than using the DT. The reason is like follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "what decision tree did is to find the minimum entropy in every step, but it is very easy to make overfit if training all the data at once. And the performance of decision tree is good when the data is very large.\n",
    "GaussianNB use the maximum likelihood to predict parameters and it assume the features are not related. And it have a good estimate when the data the relatively small.\n",
    "Back to our training data, because we use the 10-fold cv and the data amount for the decision tree is not large enough, so it have a lower precision and accuracy. And NB have a better result as same time.\n",
    "And for the recall. NB is not like DT that trying have a minimum entroy, so it make cause more FP or FN when using that classifier, which may make the recall is a little lower than the DT classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### b.\tUse LinearSVC to learn a linear Support Vector Machine model to predict highCrime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i.\t What is the 10-fold cross-validation accuracy, precision, and recall for this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10 cv is:\n",
      "[ 0.79        0.865       0.84        0.80904523  0.69346734  0.69346734\n",
      "  0.81407035  0.8241206   0.81909548  0.81407035]\n",
      "the precision for 10 cv is:\n",
      "[ 0.76774194  0.84507042  0.82517483  0.85950413  0.87209302  0.7962963\n",
      "  0.84920635  0.95918367  0.80689655  0.87288136]\n",
      "the recall for 10 cv is:\n",
      "[ 0.952  0.96   0.944  0.832  0.6    0.688  0.856  0.752  0.936  0.824]\n",
      "the avg accuracy is: 0.796233668342\n",
      "the avg precision is: 0.845404856531\n",
      "the avg recall is: 0.8344\n"
     ]
    }
   ],
   "source": [
    "#init linear svm\n",
    "ln_svm_clf = LinearSVC()\n",
    "ln_svm_clf.fit(clean_X,clean_clf_y)\n",
    "ln_svm_clf_accuracy = cross_val_score(ln_svm_clf, clean_X, clean_clf_y, cv=10,scoring='accuracy')\n",
    "ln_svm_clf_precision = cross_val_score(ln_svm_clf, clean_X, clean_clf_y, cv=10,scoring='precision')\n",
    "ln_svm_clf_recall = cross_val_score(ln_svm_clf, clean_X, clean_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10 cv is:\")\n",
    "print(ln_svm_clf_accuracy)\n",
    "print(\"the precision for 10 cv is:\")\n",
    "print(ln_svm_clf_precision)\n",
    "print(\"the recall for 10 cv is:\")\n",
    "print(ln_svm_clf_recall)\n",
    "print(\"the avg accuracy is:\",sum(ln_svm_clf_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(ln_svm_clf_precision)/10)\n",
    "print(\"the avg recall is:\",sum(ln_svm_clf_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii.\tWhat are the 10 most predictive features? This can be measured by the absolute feature weights in the model. Why do these make sense (or not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pctWInvInc 1.88848797592\n",
      "PersPerOccupHous 1.75512340176\n",
      "racePctWhite 1.50021823724\n",
      "PctKids2Par 1.19032762621\n",
      "RentHighQ 1.06688182158\n",
      "MalePctDivorce 1.06569629959\n",
      "NumUnderPov 1.0515475679\n",
      "NumStreet 1.0191545849\n",
      "PctOccupMgmtProf 1.01467446351\n",
      "population 1.0023018298\n",
      "agePct12t21 0.988272831846\n"
     ]
    }
   ],
   "source": [
    "ln_svm_feature_value = abs(ln_svm_clf.coef_[0])\n",
    "ln_svm_main_feature = dict(zip(clean_features,ln_svm_feature_value))\n",
    "for index,w in enumerate(sorted(ln_svm_main_feature, key=ln_svm_main_feature.get, reverse=True)):\n",
    "    print(w ,ln_svm_main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the result is make sense, this is because the final predict for the linear svm is the combination of features with there weights as coefficients, so the bigger the weights, the more influence on the final result. So the features rank by the features weight is make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "iii.How do these results compare with your results from decision trees above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "most main features is similiar with the decision tree, with a slight difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the recall, precision and accuracy in linear svm are all better than in the decision tree classifier. this may because the data is not large enought for the decision tree to make a good predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### a.\tUse LinearRegression to learn a linear model directly predicting the crime rate per capita (ViolentCrimesPerPop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i.\tUsing 10-fold cross-validation, what is the estimated mean-squared-error (MSE) of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 10-fold CV MSE for linear regression is:\n",
      "[-0.02983448 -0.02048526 -0.02735939 -0.02320408 -0.01944074 -0.01262621\n",
      " -0.01844842 -0.01222962 -0.02183042 -0.01548108]\n"
     ]
    }
   ],
   "source": [
    "#init linear regression\n",
    "LR = LinearRegression()\n",
    "LR.fit(clean_X,clean_reg_y)\n",
    "LR_cv_MSE = cross_val_score(LR, clean_X, clean_reg_y, cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"the 10-fold CV MSE for linear regression is:\")\n",
    "print(LR_cv_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii.What is the MSE on the training set (train on all the data then test on it all)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the MSE for linear regression is:\n",
      "0.0165167748803\n"
     ]
    }
   ],
   "source": [
    "LR_MSE = mean_squared_error(clean_reg_y, LR.predict(clean_X))\n",
    "print(\"the MSE for linear regression is:\")\n",
    "print(LR_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "iii. What features are most predictive of a high crime rate? A low crime rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PersPerOccupHous 0.635088116499\n",
      "PctHousOwnOcc 0.568133209887\n",
      "MalePctDivorce 0.458517048642\n",
      "PctRecImmig8 0.432510557653\n",
      "MedRent 0.372727797711\n",
      "medFamInc 0.287978874946\n",
      "PctEmploy 0.248474316085\n",
      "MalePctNevMarr 0.226727912842\n",
      "PctPersDenseHous 0.214352564142\n",
      "OwnOccMedVal 0.212875863654\n",
      "racepctblack 0.204933833659\n"
     ]
    }
   ],
   "source": [
    "LR_main_feature = dict(zip(clean_features,LR.coef_))\n",
    "#the high crime rate features\n",
    "for index,w in enumerate(sorted(LR_main_feature, key=LR_main_feature.get, reverse=True)):\n",
    "    print(w ,LR_main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PctPersOwnOccup -0.675694478803\n",
      "TotalPctDiv -0.561924314415\n",
      "whitePerCap -0.351015774441\n",
      "PctKids2Par -0.322651276496\n",
      "OwnOccLowQuart -0.308170219193\n",
      "numbUrban -0.296442543178\n",
      "PersPerRentOccHous -0.254571662716\n",
      "RentLowQ -0.234751516303\n",
      "agePct12t29 -0.229217836999\n",
      "PctRecImmig5 -0.218221007355\n",
      "pctWWage -0.201573959743\n"
     ]
    }
   ],
   "source": [
    "#the low crime rate features\n",
    "for index,w in enumerate(sorted(LR_main_feature, key=LR_main_feature.get, reverse=False)):\n",
    "    print(w ,LR_main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PersPerOccupHous predict a high crime rate, PctPersOwnOccup predict a low crime rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### b.\tNow use Ridge regression to reduce the amount of overfitting, using RidgeCV to pick the best alpha from among (10, 1, 0.1, 0.01, and 0.001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i.\tWhat is the estimated MSE of the model under 10-fold CV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 10-fold cv MSE for ridge regression is:\n",
      "[-0.0295138  -0.01972075 -0.02795035 -0.02311508 -0.01872138 -0.0120445\n",
      " -0.01780565 -0.01212225 -0.02191791 -0.01503854]\n"
     ]
    }
   ],
   "source": [
    "#we use the default parameter(alpha = 1.0) to init first\n",
    "RG = Ridge()\n",
    "RG.fit(clean_X, clean_reg_y)\n",
    "RG_cv_MSE = cross_val_score(RG, clean_X, clean_reg_y, cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"the 10-fold cv MSE for ridge regression is:\")\n",
    "print(RG_cv_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii.What is the MSE on the training set (train on all the data then test on it all)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the MSE for ridge regression is:\n",
      "0.0167635291552\n"
     ]
    }
   ],
   "source": [
    "RG_MSE = mean_squared_error(clean_reg_y, RG.predict(clean_X))\n",
    "print(\"the MSE for ridge regression is:\")\n",
    "print(RG_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "iii.What is the best alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best alpha is:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "RG_cv = RidgeCV(alphas = np.array([10,1,0.1,0.01,0.001]), cv = 10)\n",
    "RG_cv.fit(clean_X,clean_reg_y)\n",
    "print(\"the best alpha is:\")\n",
    "print(RG_cv.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "because the default alpha is also 1.0, so we don't need to calculate the 10-fold CV MSE and the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "iv.What does this say about the amount of overfitting in linear regression for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "we can find that the 10-fold MSE in ridge regression is less than using the linear regression. This is because the ridge regression have give more penality to the unimportant features, which make the overfit of the ridge regression decrease. So the ridge regression have a smaller 10-fold cv MSE than the linear regression's 10-fold cv MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### c.\tNow use polynomial features to do quadratic (second-order) polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "we need to prepare the data again to do the polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#using the PolynomialFeatures to transform the data\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "poly_clean_X = poly.fit_transform(clean_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i.What is the estimated MSE of the model under 10-fold CV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 10-fold cv poly regression MSE is:\n",
      "[-0.15479254 -0.1498775  -0.19852639 -0.1021161  -0.07857951 -0.1410196\n",
      " -0.09207064 -0.0737748  -0.1692134  -0.13903143]\n"
     ]
    }
   ],
   "source": [
    "#init the poly regression\n",
    "poly_LR = LinearRegression()\n",
    "poly_LR.fit(poly_clean_X,clean_reg_y)\n",
    "poly_LR_cv_MSE = cross_val_score(poly_LR, poly_clean_X, clean_reg_y, cv=10,scoring='neg_mean_squared_error')\n",
    "print(\"the 10-fold cv poly regression MSE is:\")\n",
    "print(poly_LR_cv_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii.What is the MSE on the training set (train on all the data then test on it all)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the poly regression MSE is:\n",
      "6.91579443622e-29\n"
     ]
    }
   ],
   "source": [
    "poly_LR_MSE = mean_squared_error(clean_reg_y, poly_LR.predict(poly_clean_X))\n",
    "print(\"the poly regression MSE is:\")\n",
    "print(poly_LR_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "iii.Does this mean the quadratic model is better than the linear model for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the result is not better than the linear regression model, because it is overfit, it makes the 10-fold cv MSE larger than the linear regression one, and because of the overfit, it make the full data MSE very very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. Dirty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Repeat the decision tree learning question for the full (non-clean) data set and present the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### a. get the crime rate and calculate the percentage of positive and negative example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the positve percentage is:\n",
      "0.627382146439318\n",
      "the negative percentage is:\n",
      "0.37261785356068206\n"
     ]
    }
   ],
   "source": [
    "#the highCrime is the same as clean_clf_y\n",
    "full_highCrime = list(full_clf_y)\n",
    "#the positive number and neg number is \n",
    "full_pos = full_highCrime.count(1)\n",
    "full_neg = full_highCrime.count(0)\n",
    "#the percentage\n",
    "print(\"the positve percentage is:\")\n",
    "print(full_pos/len(full_highCrime))\n",
    "print(\"the negative percentage is:\")\n",
    "print(full_neg/len(full_highCrime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### b.  Use DecisionTreeClassifier to learn a decision tree to predict highCrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#init the full DT clf\n",
    "full_dt_clf = DecisionTreeClassifier()\n",
    "#fit the data\n",
    "full_dt_clf.fit(full_X,full_clf_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i. the accuracy, precision and recall are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy is  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       1.00      1.00      1.00       743\n",
      "   positive       1.00      1.00      1.00      1251\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_answer = full_dt_clf.predict_proba(full_X)[:,1]  \n",
    "full_accuracy = accuracy_score(full_clf_y, full_dt_clf.predict(full_X))  \n",
    "print(\"the accuracy is \",full_accuracy)\n",
    "print(classification_report(full_clf_y,full_answer, target_names = ['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii. now let's find the main features according to the gini importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PctKids2Par 0.358795967643\n",
      "racePctWhite 0.086480363916\n",
      "racePctHisp 0.0505171773405\n",
      "MedRent 0.0191605094703\n",
      "PctEmplManu 0.0191288131366\n",
      "PctLess9thGrade 0.0187335227013\n",
      "PctEmploy 0.0162205304653\n",
      "PctSpeakEnglOnly 0.0149754941779\n",
      "blackPerCap 0.0148174881971\n",
      "PctImmigRec10 0.0143954680319\n",
      "PctSameState85 0.012215528406\n"
     ]
    }
   ],
   "source": [
    "full_main_feature = dict(zip(full_features,full_dt_clf.feature_importances_))\n",
    "for index,w in enumerate(sorted(full_main_feature, key=full_main_feature.get, reverse=True)):\n",
    "    print(w ,full_main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above result is make sense, because the decision tree is create by get the minimum entropy at every decision, and from the data we can find that if we order the data with the main features above, they are all high related to the crime rate. So the above result make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### c. Now apply cross-validation (cross_val_score) to do 10-fold cross-validation to estimate the out-of-training accuracy of decision tree learning for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i. What are the 10-fold cross-validation accuracy, precision, and recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10-fold cv is:\n",
      "[ 0.80597015  0.745       0.735       0.75376884  0.72361809  0.75376884\n",
      "  0.80904523  0.75879397  0.74371859  0.74371859]\n",
      "the precision for 10-fold cv is:\n",
      "[ 0.85        0.81746032  0.8         0.80645161  0.79831933  0.81512605\n",
      "  0.8699187   0.80672269  0.79130435  0.7804878 ]\n",
      "the recall for 10-fold cv is:\n",
      "[ 0.81746032  0.808       0.744       0.848       0.768       0.784       0.84\n",
      "  0.752       0.76        0.784     ]\n"
     ]
    }
   ],
   "source": [
    "full_ten_fold_accuracy = cross_val_score(full_dt_clf, full_X, full_clf_y, cv=10,scoring='accuracy')\n",
    "full_ten_fold_precision = cross_val_score(full_dt_clf, full_X, full_clf_y, cv=10,scoring='precision')\n",
    "full_ten_fold_recall = cross_val_score(full_dt_clf, full_X, full_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10-fold cv is:\")\n",
    "print(full_ten_fold_accuracy)\n",
    "print(\"the precision for 10-fold cv is:\")\n",
    "print(full_ten_fold_precision)\n",
    "print(\"the recall for 10-fold cv is:\")\n",
    "print(full_ten_fold_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii.Why are they different from the results in the previous test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "there are several reasons to cause this result\n",
    "        1. we do not doing the feature reduction or layer limit for the decision tree, so when we doing the dt training for the all data, we have the overfit which make all the result is 100%.\n",
    "        2. for the 10-fold part, because we doing the corss validation, so it will decrease the rate of overfit, which we also decrease the accuracy, precasion, recall for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### additional question :\n",
    "#### a.\tAre the CV results better or worse? What does this say about the effect of missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the CV result is better than the clean data one, this means that some parameters which misssing value are also important features that influence the predict of the decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.\tTeams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "because we are a two-member team, so we will do the part a, and part b will for extra credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "####  a. If you are working in a team of two people:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i.\tExperiment with two learning methods other than those described above (one can be a non-linear kernel for SVM) for the classification problem, explaining clearly what you did. Show CV results for both the clean and full datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### using the SGDClassifier first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10-fold cv is:\n",
      "[ 0.74        0.845       0.84        0.85427136  0.65326633  0.70854271\n",
      "  0.81909548  0.87437186  0.82914573  0.72361809]\n",
      "the precision for 10-fold cv is:\n",
      "[ 0.70833333  0.7251462   0.7852349   0.69273743  0.96226415  0.75172414\n",
      "  0.9245283   1.          0.89247312  0.76      ]\n",
      "the recall for 10-fold cv is:\n",
      "[ 0.88   0.592  1.     0.992  0.424  0.648  0.824  0.736  0.792  0.648]\n"
     ]
    }
   ],
   "source": [
    "#init the sgd_clf for clean data\n",
    "clean_sgd_clf = SGDClassifier()\n",
    "sgd_clean_ten_fold_accuracy = cross_val_score(clean_sgd_clf, clean_X, clean_clf_y, cv=10,scoring='accuracy')\n",
    "sgd_clean_ten_fold_precision = cross_val_score(clean_sgd_clf, clean_X, clean_clf_y, cv=10,scoring='precision')\n",
    "sgd_clean_ten_fold_recall = cross_val_score(clean_sgd_clf, clean_X, clean_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10-fold cv is:\")\n",
    "print(sgd_clean_ten_fold_accuracy)\n",
    "print(\"the precision for 10-fold cv is:\")\n",
    "print(sgd_clean_ten_fold_precision)\n",
    "print(\"the recall for 10-fold cv is:\")\n",
    "print(sgd_clean_ten_fold_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10-fold cv is:\n",
      "[ 0.64179104  0.805       0.795       0.72864322  0.79899497  0.79396985\n",
      "  0.86934673  0.84422111  0.79899497  0.71356784]\n",
      "the precision for 10-fold cv is:\n",
      "[ 0.67021277  0.73053892  0.65608466  0.80985915  0.94186047  0.72093023\n",
      "  0.82191781  0.8699187   0.98684211  0.88596491]\n",
      "the recall for 10-fold cv is:\n",
      "[ 1.     0.672  0.76   0.744  0.912  0.736  0.592  0.888  0.632  0.912]\n"
     ]
    }
   ],
   "source": [
    "#init the sgd_clf for full data\n",
    "full_sgd_clf = SGDClassifier()\n",
    "sgd_full_ten_fold_accuracy = cross_val_score(full_sgd_clf, full_X, full_clf_y, cv=10,scoring='accuracy')\n",
    "sgd_full_ten_fold_precision = cross_val_score(full_sgd_clf, full_X, full_clf_y, cv=10,scoring='precision')\n",
    "sgd_full_ten_fold_recall = cross_val_score(full_sgd_clf, full_X, full_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10-fold cv is:\")\n",
    "print(sgd_full_ten_fold_accuracy)\n",
    "print(\"the precision for 10-fold cv is:\")\n",
    "print(sgd_full_ten_fold_precision)\n",
    "print(\"the recall for 10-fold cv is:\")\n",
    "print(sgd_full_ten_fold_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### using the RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10-fold cv is:\n",
      "[ 0.81        0.89        0.81        0.79899497  0.71356784  0.73869347\n",
      "  0.82914573  0.77889447  0.82914573  0.81407035]\n",
      "the precision for 10-fold cv is:\n",
      "[ 0.75        0.86507937  0.84558824  0.864       0.86170213  0.87368421\n",
      "  0.82706767  0.9         0.85294118  0.86065574]\n",
      "the recall for 10-fold cv is:\n",
      "[ 0.912  0.928  0.896  0.864  0.616  0.656  0.856  0.672  0.912  0.808]\n"
     ]
    }
   ],
   "source": [
    "#init the rf_clf for clean data\n",
    "clean_rf_clf = RandomForestClassifier()\n",
    "rf_clean_ten_fold_accuracy = cross_val_score(clean_rf_clf, clean_X, clean_clf_y, cv=10,scoring='accuracy')\n",
    "rf_clean_ten_fold_precision = cross_val_score(clean_rf_clf, clean_X, clean_clf_y, cv=10,scoring='precision')\n",
    "rf_clean_ten_fold_recall = cross_val_score(clean_rf_clf, clean_X, clean_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10-fold cv is:\")\n",
    "print(rf_clean_ten_fold_accuracy)\n",
    "print(\"the precision for 10-fold cv is:\")\n",
    "print(rf_clean_ten_fold_precision)\n",
    "print(\"the recall for 10-fold cv is:\")\n",
    "print(rf_clean_ten_fold_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10-fold cv is:\n",
      "[ 0.83084577  0.845       0.795       0.7638191   0.79396985  0.78894472\n",
      "  0.81909548  0.82914573  0.81407035  0.84924623]\n",
      "the precision for 10-fold cv is:\n",
      "[ 0.875       0.828125    0.83969466  0.84552846  0.86956522  0.85217391\n",
      "  0.8907563   0.89473684  0.85950413  0.85950413]\n",
      "the recall for 10-fold cv is:\n",
      "[ 0.84920635  0.848       0.848       0.792       0.824       0.784       0.792\n",
      "  0.84        0.832       0.848     ]\n"
     ]
    }
   ],
   "source": [
    "#init the rf_clf for full data\n",
    "full_rf_clf = RandomForestClassifier()\n",
    "rf_full_ten_fold_accuracy = cross_val_score(full_rf_clf, full_X, full_clf_y, cv=10,scoring='accuracy')\n",
    "rf_full_ten_fold_precision = cross_val_score(full_rf_clf, full_X, full_clf_y, cv=10,scoring='precision')\n",
    "rf_full_ten_fold_recall = cross_val_score(full_rf_clf, full_X, full_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10-fold cv is:\")\n",
    "print(rf_full_ten_fold_accuracy)\n",
    "print(\"the precision for 10-fold cv is:\")\n",
    "print(rf_full_ten_fold_precision)\n",
    "print(\"the recall for 10-fold cv is:\")\n",
    "print(rf_full_ten_fold_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii.\tWhat method gives the best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg accuracy,precision and recall for sgd_clf with clean data are:\n",
      "0.788731155779 0.82024415707 0.7536\n",
      "the avg accuracy,precision and recall for sgd_clf with full data are:\n",
      "0.778952973824 0.809412972175 0.7848\n",
      "the avg accuracy,precision and recall for rf_clf with clean data are:\n",
      "0.801251256281 0.850071852191 0.812\n",
      "the avg accuracy,precision and recall for rf_clf with full data are:\n",
      "0.812913722843 0.86145886513 0.825720634921\n"
     ]
    }
   ],
   "source": [
    "print(\"the avg accuracy,precision and recall for sgd_clf with clean data are:\")\n",
    "print(sum(sgd_clean_ten_fold_accuracy)/10,sum(sgd_clean_ten_fold_precision)/10,sum(sgd_clean_ten_fold_recall)/10)\n",
    "print(\"the avg accuracy,precision and recall for sgd_clf with full data are:\")\n",
    "print(sum(sgd_full_ten_fold_accuracy)/10,sum(sgd_full_ten_fold_precision)/10,sum(sgd_full_ten_fold_recall)/10)\n",
    "print(\"the avg accuracy,precision and recall for rf_clf with clean data are:\")\n",
    "print(sum(rf_clean_ten_fold_accuracy)/10,sum(rf_clean_ten_fold_precision)/10,sum(rf_clean_ten_fold_recall)/10)\n",
    "print(\"the avg accuracy,precision and recall for rf_clf with full data are:\")\n",
    "print(sum(rf_full_ten_fold_accuracy)/10,sum(rf_full_ten_fold_precision)/10,sum(rf_full_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "from the result above we can find that when we using the full data, we can get a better result in SGD classifier and in random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Besides, we can find that the random forest classifier have a better result than the SGD classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "iii.What feature(s) seem to be most consistently predictive of high crime rates? How reliable is this conclusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "we can use SGD coef to get the features can predict high crime rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "racepctblack 18.1238034461\n",
      "racePctHisp 10.5296745373\n",
      "MalePctDivorce 10.2288266934\n",
      "PctIlleg 9.16218433768\n",
      "TotalPctDiv 8.24140760325\n",
      "PctSameCity85 8.03172577263\n",
      "PctPersDenseHous 7.82204394202\n",
      "OtherPerCap 7.57589570608\n",
      "FemalePctDiv 5.9075576625\n",
      "HousVacant 5.55201021059\n",
      "MedRentPctHousInc 5.43349439329\n"
     ]
    }
   ],
   "source": [
    "full_sgd_clf.fit(full_X,full_clf_y)\n",
    "full_sgd_main_feature = dict(zip(full_features,full_sgd_clf.coef_[0]))\n",
    "for index,w in enumerate(sorted(full_sgd_main_feature, key=full_sgd_main_feature.get, reverse=True)):\n",
    "    print(w ,full_sgd_main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "we can also use random forest feature importance to get the features have heavy effect on the crime rate, but we cannot get weather the feature have positive influence or negative influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main features of random forest for the first test\n",
      "FemalePctDiv 0.0820288699727\n",
      "PctFam2Par 0.0701253187026\n",
      "PctYoungKids2Par 0.0645716820006\n",
      "PctPopUnderPov 0.0564569707245\n",
      "NumIlleg 0.046618267516\n",
      "PctIlleg 0.0271528877738\n",
      "PctPersDenseHous 0.0262136831012\n",
      "racepctblack 0.0254071802517\n",
      "racePctWhite 0.0241139563332\n",
      "racePctHisp 0.0184378345414\n",
      "PctLargHouseFam 0.0183883871743\n"
     ]
    }
   ],
   "source": [
    "full_rf_clf.fit(full_X,full_clf_y)\n",
    "full_rf_main_feature = dict(zip(full_features,full_rf_clf.feature_importances_))\n",
    "print(\"the main features of random forest for the first test\")\n",
    "for index,w in enumerate(sorted(full_rf_main_feature, key=full_rf_main_feature.get, reverse=True)):\n",
    "    print(w ,full_rf_main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main features of random forest for the second test\n",
      "PctKids2Par 0.0950173078848\n",
      "racePctWhite 0.0790165718385\n",
      "FemalePctDiv 0.0785059153857\n",
      "pctWInvInc 0.0320892894463\n",
      "PctPersOwnOccup 0.0298088513386\n",
      "NumUnderPov 0.0271243409871\n",
      "PctPersDenseHous 0.0234306880347\n",
      "PctHousOwnOcc 0.0204980193541\n",
      "PctTeen2Par 0.019813402429\n",
      "PctIlleg 0.0193924714482\n",
      "PctFam2Par 0.0175810364368\n"
     ]
    }
   ],
   "source": [
    "full_rf_clf.fit(full_X,full_clf_y)\n",
    "full_rf_main_feature = dict(zip(full_features,full_rf_clf.feature_importances_))\n",
    "print(\"the main features of random forest for the second test\")\n",
    "for index,w in enumerate(sorted(full_rf_main_feature, key=full_rf_main_feature.get, reverse=True)):\n",
    "    print(w ,full_rf_main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "actually we can find that the main features we get from the random forest is not stable.\n",
    "Although the random forest can have a good predict of the crime, the result of main features in random forest is not good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the result of the main features using SGD_classifier seems stable. Because the SGD_classifier using the Gradient Descent to find the best weight for every parameter. Hence the result should be reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "But, it also have limits, because our data is not enough(only nearly 2K), we can only confirm that the random forest and SGD can fit our training data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "back to our data, the most important feature that predict high crime is racepctblack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6. Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### a. Do a team requirement (above) that your team is not already required to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i. Do the requirement for two person teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the result is already show above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii. Devise a method to find the most useful threshold for dividing high crime areas from low crime areas (i.e., discretizing XXX to compute highCrime). Define clearly what you mean by useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "we can using k-means algorithm to find the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new threshold is: [ 0.38541353]\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=2)\n",
    "tmp = full_reg_y.copy()\n",
    "tmp = tmp.values.reshape(-1,1)\n",
    "km.fit(tmp)\n",
    "threshold = sum(km.cluster_centers_)/2\n",
    "print(\"the new threshold is:\", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "this threshold is useful because it using the k-means algorithm to find the cluster centers and using the middle point as the threshold. If the algorithm is correct, it should be able to increase the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "iii. Show CV results for both the clean and full datasets for at least three different classification methods above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### before doing the classification, we need to create the new label for the new classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_clean_clf_y = clean_reg_y.copy()\n",
    "new_full_clf_y = full_reg_y.copy()\n",
    "for index, item in enumerate(new_clean_clf_y):\n",
    "    if (item > threshold):\n",
    "        new_clean_clf_y[index] = 1\n",
    "    else:\n",
    "        new_clean_clf_y[index] = 0\n",
    "for index, item in enumerate(new_full_clf_y):\n",
    "    if (item > threshold):\n",
    "        new_full_clf_y[index] = 1\n",
    "    else:\n",
    "        new_full_clf_y[index] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### we will using decision tree, GaussianNB and LinearSVC to test the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### for the clean data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10-fold cv is:\n",
      "[ 0.75124378  0.84        0.70351759  0.83919598  0.83417085  0.7839196\n",
      "  0.85929648  0.85427136  0.61809045  0.73366834]\n",
      "the precision for 10-fold cv is:\n",
      "[ 0.44444444  0.61111111  0.38888889  0.6969697   0.55        0.5\n",
      "  0.63414634  0.65789474  0.3164557   0.425     ]\n",
      "the recall for 10-fold cv is:\n",
      "[ 0.58536585  0.55        0.575       0.55        0.6         0.525       0.675\n",
      "  0.625       0.625       0.375     ]\n",
      "the avg accuracy is: 0.781737443436\n",
      "the avg precision is: 0.522491091592\n",
      "the avg recall is: 0.568536585366\n"
     ]
    }
   ],
   "source": [
    "#using the dt\n",
    "new_dt_clf = DecisionTreeClassifier()\n",
    "new_ten_fold_accuracy = cross_val_score(new_dt_clf, clean_X, new_clean_clf_y, cv=10,scoring='accuracy')\n",
    "new_ten_fold_precision = cross_val_score(new_dt_clf, clean_X, new_clean_clf_y, cv=10,scoring='precision')\n",
    "new_ten_fold_recall = cross_val_score(new_dt_clf, clean_X, new_clean_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10-fold cv is:\")\n",
    "print(new_ten_fold_accuracy)\n",
    "print(\"the precision for 10-fold cv is:\")\n",
    "print(new_ten_fold_precision)\n",
    "print(\"the recall for 10-fold cv is:\")\n",
    "print(new_ten_fold_recall)\n",
    "print(\"the avg accuracy is:\",sum(new_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(new_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(new_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10 cv is:\n",
      "[ 0.75124378  0.825       0.84422111  0.88442211  0.85427136  0.83919598\n",
      "  0.87437186  0.87939698  0.72361809  0.83417085]\n",
      "the precision for 10 cv is:\n",
      "[ 0.44        0.54237288  0.57142857  0.71794872  0.62222222  0.6\n",
      "  0.63636364  0.69047619  0.41176471  0.57142857]\n",
      "the recall for 10 cv is:\n",
      "[ 0.80487805  0.8         0.9         0.7         0.7         0.6         0.875\n",
      "  0.725       0.875       0.7       ]\n",
      "the avg accuracy is: 0.83099121228\n",
      "the avg precision is: 0.580400549711\n",
      "the avg recall is: 0.767987804878\n"
     ]
    }
   ],
   "source": [
    "#using the gaussian NB\n",
    "new_nb_clf = GaussianNB()\n",
    "new_nb_clf_accuracy = cross_val_score(new_nb_clf, clean_X, new_clean_clf_y, cv=10,scoring='accuracy')\n",
    "new_nb_clf_precision = cross_val_score(new_nb_clf, clean_X, new_clean_clf_y, cv=10,scoring='precision')\n",
    "new_nb_clf_recall = cross_val_score(new_nb_clf, clean_X, new_clean_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10 cv is:\")\n",
    "print(new_nb_clf_accuracy)\n",
    "print(\"the precision for 10 cv is:\")\n",
    "print(new_nb_clf_precision)\n",
    "print(\"the recall for 10 cv is:\")\n",
    "print(new_nb_clf_recall)\n",
    "print(\"the avg accuracy is:\",sum(new_nb_clf_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(new_nb_clf_precision)/10)\n",
    "print(\"the avg recall is:\",sum(new_nb_clf_recall)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10 cv is:\n",
      "[ 0.85074627  0.89        0.8040201   0.86432161  0.85929648  0.85929648\n",
      "  0.89949749  0.89447236  0.73869347  0.81909548]\n",
      "the precision for 10 cv is:\n",
      "[ 0.63414634  0.76470588  0.51282051  0.72413793  0.66666667  0.8\n",
      "  0.83333333  0.88        0.41176471  0.6       ]\n",
      "the recall for 10 cv is:\n",
      "[ 0.63414634  0.65        0.5         0.525       0.6         0.4         0.625\n",
      "  0.55        0.7         0.3       ]\n",
      "the avg accuracy is: 0.847943973599\n",
      "the avg precision is: 0.682757537355\n",
      "the avg recall is: 0.548414634146\n"
     ]
    }
   ],
   "source": [
    "#using linear svm\n",
    "new_ln_svm_clf = LinearSVC()\n",
    "new_ln_svm_clf_accuracy = cross_val_score(new_ln_svm_clf, clean_X, new_clean_clf_y, cv=10,scoring='accuracy')\n",
    "new_ln_svm_clf_precision = cross_val_score(new_ln_svm_clf, clean_X, new_clean_clf_y, cv=10,scoring='precision')\n",
    "new_ln_svm_clf_recall = cross_val_score(new_ln_svm_clf, clean_X, new_clean_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10 cv is:\")\n",
    "print(new_ln_svm_clf_accuracy)\n",
    "print(\"the precision for 10 cv is:\")\n",
    "print(new_ln_svm_clf_precision)\n",
    "print(\"the recall for 10 cv is:\")\n",
    "print(new_ln_svm_clf_recall)\n",
    "print(\"the avg accuracy is:\",sum(new_ln_svm_clf_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(new_ln_svm_clf_precision)/10)\n",
    "print(\"the avg recall is:\",sum(new_ln_svm_clf_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### for the dirty data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10-fold cv is:\n",
      "[ 0.82089552  0.78        0.835       0.8040201   0.8241206   0.81909548\n",
      "  0.83919598  0.81407035  0.87437186  0.84422111]\n",
      "the precision for 10-fold cv is:\n",
      "[ 0.51351351  0.40909091  0.53846154  0.58333333  0.57894737  0.48888889\n",
      "  0.56521739  0.56097561  0.65        0.62162162]\n",
      "the recall for 10-fold cv is:\n",
      "[ 0.46341463  0.475       0.75        0.7         0.525       0.55        0.725\n",
      "  0.55        0.7         0.575     ]\n",
      "the avg accuracy is: 0.825499099977\n",
      "the avg precision is: 0.551005017439\n",
      "the avg recall is: 0.601341463415\n"
     ]
    }
   ],
   "source": [
    "#using the dt\n",
    "full_new_dt_clf = DecisionTreeClassifier()\n",
    "full_new_ten_fold_accuracy = cross_val_score(full_new_dt_clf, full_X, new_full_clf_y, cv=10,scoring='accuracy')\n",
    "full_new_ten_fold_precision = cross_val_score(full_new_dt_clf, full_X, new_full_clf_y, cv=10,scoring='precision')\n",
    "full_new_ten_fold_recall = cross_val_score(full_new_dt_clf, full_X, new_full_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10-fold cv is:\")\n",
    "print(full_new_ten_fold_accuracy)\n",
    "print(\"the precision for 10-fold cv is:\")\n",
    "print(full_new_ten_fold_precision)\n",
    "print(\"the recall for 10-fold cv is:\")\n",
    "print(full_new_ten_fold_recall)\n",
    "print(\"the avg accuracy is:\",sum(full_new_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(full_new_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(full_new_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10 cv is:\n",
      "[ 0.7960199   0.815       0.805       0.83417085  0.84924623  0.85427136\n",
      "  0.84422111  0.7839196   0.81407035  0.85929648]\n",
      "the precision for 10 cv is:\n",
      "[ 0.5         0.53658537  0.51111111  0.56603774  0.63157895  0.62222222\n",
      "  0.61538462  0.46666667  0.53333333  0.65789474]\n",
      "the recall for 10 cv is:\n",
      "[ 0.58536585  0.55        0.575       0.75        0.6         0.7         0.6\n",
      "  0.525       0.6         0.625     ]\n",
      "the avg accuracy is: 0.82552158804\n",
      "the avg precision is: 0.564081473463\n",
      "the avg recall is: 0.611036585366\n"
     ]
    }
   ],
   "source": [
    "#using the gaussian NB\n",
    "full_new_nb_clf = GaussianNB()\n",
    "full_new_nb_clf_accuracy = cross_val_score(full_new_nb_clf, full_X, new_full_clf_y, cv=10,scoring='accuracy')\n",
    "full_new_nb_clf_precision = cross_val_score(full_new_nb_clf, full_X, new_full_clf_y, cv=10,scoring='precision')\n",
    "full_new_nb_clf_recall = cross_val_score(full_new_nb_clf, full_X, new_full_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10 cv is:\")\n",
    "print(full_new_nb_clf_accuracy)\n",
    "print(\"the precision for 10 cv is:\")\n",
    "print(full_new_nb_clf_precision)\n",
    "print(\"the recall for 10 cv is:\")\n",
    "print(full_new_nb_clf_recall)\n",
    "print(\"the avg accuracy is:\",sum(full_new_nb_clf_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(full_new_nb_clf_precision)/10)\n",
    "print(\"the avg recall is:\",sum(full_new_nb_clf_recall)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10 cv is:\n",
      "[ 0.85572139  0.885       0.88        0.88944724  0.86432161  0.90954774\n",
      "  0.90452261  0.86934673  0.87939698  0.87939698]\n",
      "the precision for 10 cv is:\n",
      "[ 0.77272727  0.74285714  0.76666667  0.76470588  0.6969697   0.86666667\n",
      "  0.81818182  0.75        0.72222222  0.76666667]\n",
      "the recall for 10 cv is:\n",
      "[ 0.41463415  0.65        0.575       0.65        0.575       0.65        0.675\n",
      "  0.525       0.65        0.575     ]\n",
      "the avg accuracy is: 0.881670129253\n",
      "the avg precision is: 0.766766403531\n",
      "the avg recall is: 0.593963414634\n"
     ]
    }
   ],
   "source": [
    "#using linear svm\n",
    "full_new_ln_svm_clf = LinearSVC()\n",
    "full_new_ln_svm_clf_accuracy = cross_val_score(full_new_ln_svm_clf, full_X, new_full_clf_y, cv=10,scoring='accuracy')\n",
    "full_new_ln_svm_clf_precision = cross_val_score(full_new_ln_svm_clf, full_X, new_full_clf_y, cv=10,scoring='precision')\n",
    "full_new_ln_svm_clf_recall = cross_val_score(full_new_ln_svm_clf, full_X, new_full_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10 cv is:\")\n",
    "print(full_new_ln_svm_clf_accuracy)\n",
    "print(\"the precision for 10 cv is:\")\n",
    "print(full_new_ln_svm_clf_precision)\n",
    "print(\"the recall for 10 cv is:\")\n",
    "print(full_new_ln_svm_clf_recall)\n",
    "print(\"the avg accuracy is:\",sum(full_new_ln_svm_clf_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(full_new_ln_svm_clf_precision)/10)\n",
    "print(\"the avg recall is:\",sum(full_new_ln_svm_clf_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "iv. How are these results similar and different from the previous results (with a fixed threshold of 0.1). What does this say about how to approach such a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "we can find that the result is different from the previous results. Using the new threshold increase the accuracy while decrease the precision and recall. This is because the new threshold have increase the number of negative example. Which may cause the decrease of precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "overall, the new threshold is relatively good, because it's increase the accuracy, which is the most important parameter in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "####  b. Experiment with other learning methods such as polynomial or other kernels in SVM, decision forests, boosting, etc. and show your results. Make sure to explain clearly what you did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### we analysis the classification method first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "using the method to deal the clean data first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1.we can using the adabooster method to deal with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg accuracy is: 0.782668341709\n",
      "the avg precision is: 0.837085111098\n",
      "the avg recall is: 0.8216\n"
     ]
    }
   ],
   "source": [
    "#init the adb_clf with clean data\n",
    "clean_adb_clf = AdaBoostClassifier()\n",
    "adb_clean_ten_fold_accuracy = cross_val_score(clean_adb_clf, clean_X, clean_clf_y, cv=10,scoring='accuracy')\n",
    "adb_clean_ten_fold_precision = cross_val_score(clean_adb_clf, clean_X, clean_clf_y, cv=10,scoring='precision')\n",
    "adb_clean_ten_fold_recall = cross_val_score(clean_adb_clf, clean_X, clean_clf_y, cv=10,scoring='recall')\n",
    "print(\"the avg accuracy is:\",sum(adb_clean_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(adb_clean_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(adb_clean_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.we can also using the GradientBoostingClassifier to deal with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg accuracy is: 0.798213567839\n",
      "the avg precision is: 0.850568409405\n",
      "the avg recall is: 0.8336\n"
     ]
    }
   ],
   "source": [
    "#init the gb_clf with clean data\n",
    "clean_gb_clf = GradientBoostingClassifier()\n",
    "gb_clean_ten_fold_accuracy = cross_val_score(clean_gb_clf, clean_X, clean_clf_y, cv=10,scoring='accuracy')\n",
    "gb_clean_ten_fold_precision = cross_val_score(clean_gb_clf, clean_X, clean_clf_y, cv=10,scoring='precision')\n",
    "gb_clean_ten_fold_recall = cross_val_score(clean_gb_clf, clean_X, clean_clf_y, cv=10,scoring='recall')\n",
    "print(\"the avg accuracy is:\",sum(gb_clean_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(gb_clean_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(gb_clean_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "3.we have implement the random forest at above section, the result is like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg accuracy is: 0.801251256281\n",
      "the avg precision is: 0.850071852191\n",
      "the avg recall is: 0.812\n"
     ]
    }
   ],
   "source": [
    "print(\"the avg accuracy is:\",sum(rf_clean_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(rf_clean_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(rf_clean_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "4.we have also implement the SGD_clf at above, the result is like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg accuracy is: 0.788731155779\n",
      "the avg precision is: 0.82024415707\n",
      "the avg recall is: 0.7536\n"
     ]
    }
   ],
   "source": [
    "print(\"the avg accuracy is:\",sum(sgd_clean_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(sgd_clean_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(sgd_clean_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "we can deal the full data using the above method too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg accuracy is: 0.820438898472\n",
      "the avg precision is: 0.85466453235\n",
      "the avg recall is: 0.861688888889\n"
     ]
    }
   ],
   "source": [
    "#init the full adb_clf\n",
    "full_adb_clf = AdaBoostClassifier()\n",
    "adb_full_ten_fold_accuracy = cross_val_score(full_adb_clf, full_X, full_clf_y, cv=10,scoring='accuracy')\n",
    "adb_full_ten_fold_precision = cross_val_score(full_adb_clf, full_X, full_clf_y, cv=10,scoring='precision')\n",
    "adb_full_ten_fold_recall = cross_val_score(full_adb_clf, full_X, full_clf_y, cv=10,scoring='recall')\n",
    "print(\"the avg accuracy is:\",sum(adb_full_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(adb_full_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(adb_full_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg accuracy is: 0.837989224731\n",
      "the avg precision is: 0.873403006091\n",
      "the avg recall is: 0.868095238095\n"
     ]
    }
   ],
   "source": [
    "#init the full gb_clf\n",
    "full_gb_clf = GradientBoostingClassifier()\n",
    "gb_full_ten_fold_accuracy = cross_val_score(full_gb_clf, full_X, full_clf_y, cv=10,scoring='accuracy')\n",
    "gb_full_ten_fold_precision = cross_val_score(full_gb_clf, full_X, full_clf_y, cv=10,scoring='precision')\n",
    "gb_full_ten_fold_recall = cross_val_score(full_gb_clf, full_X, full_clf_y, cv=10,scoring='recall')\n",
    "print(\"the avg accuracy is:\",sum(gb_full_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(gb_full_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(gb_full_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg accuracy is: 0.812913722843\n",
      "the avg precision is: 0.86145886513\n",
      "the avg recall is: 0.825720634921\n"
     ]
    }
   ],
   "source": [
    "#using the above full rf_clf\n",
    "print(\"the avg accuracy is:\",sum(rf_full_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(rf_full_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(rf_full_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg accuracy is: 0.778952973824\n",
      "the avg precision is: 0.809412972175\n",
      "the avg recall is: 0.7848\n"
     ]
    }
   ],
   "source": [
    "#using the above full sgd_clf\n",
    "print(\"the avg accuracy is:\",sum(sgd_full_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(sgd_full_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(sgd_full_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i.What method gives the best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "at first we can find that we can get a better result when using the full data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "we can find that when we using the GradientBoostingClassifier, we can get a really good result, it have high precision, accuracy and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii. What feature(s) seem to be most consistently predictive of high crime rates? How reliable is this conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main features of random forest for the second test\n",
      "PctKids2Par 0.0791857847178\n",
      "racePctWhite 0.0520797883371\n",
      "FemalePctDiv 0.0308962577772\n",
      "PctImmigRec10 0.0248180889751\n",
      "PctBornSameState 0.023418099002\n",
      "NumIlleg 0.0221792618535\n",
      "PctImmigRec5 0.0208748669095\n",
      "pctWInvInc 0.0208508704835\n",
      "PctVacMore6Mos 0.020319104558\n",
      "TotalPctDiv 0.0195249459073\n",
      "HousVacant 0.0193909432843\n"
     ]
    }
   ],
   "source": [
    "full_gb_clf.fit(full_X,full_clf_y)\n",
    "full_gb_main_feature = dict(zip(full_features,full_gb_clf.feature_importances_))\n",
    "print(\"the main features of random forest for the second test\")\n",
    "for index,w in enumerate(sorted(full_gb_main_feature, key=full_gb_main_feature.get, reverse=True)):\n",
    "    print(w ,full_gb_main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "from the result above, we know that the PctKids2Par have a huge influence of crime rate, but we don's know if it have positive or negative influence to the high crime rate. While from the analysis in the regresssion part, we can find that this feature have a negative influence to the high crime rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the feature is fit well for the training data we used, and according to the result of 10-fold CV, it should be fine when deal with other data. But we need to notice that our training data is only nearly 2k, so if we can gathering more data, we will get a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### we analysis the regression method next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ard regression MSE is:\n",
      "0.016838160795\n"
     ]
    }
   ],
   "source": [
    "# we using the ard regression to train the data and get the mse\n",
    "full_ard_reg = ARDRegression()\n",
    "full_ard_reg.fit(full_X,full_reg_y)\n",
    "full_ard_reg_MSE = mean_squared_error(full_reg_y, full_ard_reg.predict(full_X))\n",
    "print(\"the ard regression MSE is:\")\n",
    "print(full_ard_reg_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the lars regression MSE is:\n",
      "4.0504671199\n"
     ]
    }
   ],
   "source": [
    "# we can also using the Least Angle Regression\n",
    "full_lars_reg = Lars()\n",
    "full_lars_reg.fit(full_X,full_reg_y)\n",
    "full_lars_reg_MSE = mean_squared_error(full_reg_y, full_lars_reg.predict(full_X))\n",
    "print(\"the lars regression MSE is:\")\n",
    "print(full_lars_reg_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i. What method gives the best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "for the regression part, we can find that the MSE of ardregression is similiar with linear regression, while the lars regression donnot have a good MSE.\n",
    "combine with the regressison model at the secone part, we find that the ard regression give a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ii.What feature(s) seem to be most consistently predictive of high crime rates? How reliable is this conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main features of random forest for the second test\n",
      "population 2.55485909669\n",
      "RentHighQ 0.619174759043\n",
      "PctOccupMgmtProf 0.389099531506\n",
      "PersPerRentOccHous 0.304379272636\n",
      "racePctHisp 0.287399607682\n",
      "PctTeen2Par 0.267707033267\n",
      "MalePctDivorce 0.251091850066\n",
      "RacialMatchCommPol 0.242476105556\n",
      "perCapInc 0.158897857262\n",
      "OtherPerCap 0.136565399234\n",
      "pctUrban 0.127986059805\n"
     ]
    }
   ],
   "source": [
    "full_ard_reg_main_feature = dict(zip(full_features,full_ard_reg.coef_))\n",
    "print(\"the main features of random forest for the second test\")\n",
    "for index,w in enumerate(sorted(full_ard_reg_main_feature, key=full_ard_reg_main_feature.get, reverse=True)):\n",
    "    print(w ,full_ard_reg_main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the result is make some sense, the population is a feature of crime, but because it using the NB, actually it's assumr all the features are irrelevant. So it may cause some mistake. Besides, our training set is only 2k smaple, so we can only confirm that this model can do pretty good in our trainning set. If we can training more data, the result should be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "####  c. Find other data sets to combine with this data set that might improve results. This may include weather data (from NOAA), other demographics data from the US Census, etc. One good source for data sets is http://data.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i. Explain precisely what you did to combine the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "before i trying to find the data, i have search the original data form, i found that it contains the state number\n",
    ", community name. But when i trying to find the state naming using the community name, i find for a same community name, contains a lot of state. So i am trying to figure out what state it is in the data. By searching the documents \n",
    "of the data, i found the state number is assign by the State FIPS Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "the corresponding order are like below:\n",
    "1 Alabama\n",
    "2 Alaska\n",
    "3 American Samoa\n",
    "4 Arizona\n",
    "5 Arkansas\n",
    "6 California\n",
    "8 Colorado\n",
    "9 Connecticut\n",
    "10 Delaware\n",
    "11 District of Columbia\n",
    "12 Florida\n",
    "13 Georgia\n",
    "14 Guam\n",
    "15 Hawaii\n",
    "16 Idaho\n",
    "17 Illinois\n",
    "18 Indiana\n",
    "19 Iowa\n",
    "20 Kansas\n",
    "21 Kentucky\n",
    "22 Louisiana\n",
    "23 Maine\n",
    "24 Maryland\n",
    "25 Massachusetts\n",
    "26 Michigan\n",
    "27 Minnesota\n",
    "28 Mississippi\n",
    "29 Missouri\n",
    "30 Montana\n",
    "31 Nebraska\n",
    "32 Nevada\n",
    "33 New Hampshire\n",
    "34 New Jersey\n",
    "35 New Mexico\n",
    "36 New York\n",
    "37 North Carolina\n",
    "38 North Dakota\n",
    "39 Ohio\n",
    "40 Oklahoma\n",
    "41 Oregon\n",
    "42 Pennsylvania\n",
    "43 Puerto Rico\n",
    "44 Rhode Island\n",
    "45 South Carolina\n",
    "46 South Dakota\n",
    "47 Tennessee\n",
    "48 Texas\n",
    "49 Utah\n",
    "50 Vermont\n",
    "51 Virginia\n",
    "52 Virgin Islands\n",
    "53 Washington\n",
    "54 West Virginia\n",
    "55 Wisconsin\n",
    "56 Wyoming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "then, i am trying to find the data from the data.gov. By taking some effort, i have find the data set called \"Community Health Status Indicators (CHSI) to Combat Obesity, Heart Disease and Cancer\", the link is below:\n",
    "https://catalog.data.gov/dataset/community-health-status-indicators-chsi-to-combat-obesity-heart-disease-and-cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "i am using the csv file called \"SUMMARYMEASURESOFHEALTH.csv\" to combine my original data.\n",
    "because the cimmunity name in our original data contains some suffixes like \"city,town,township,borough,village\".\n",
    "so i deal with this suffix first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if we using the inner join, we can find the smaple number become: 162\n"
     ]
    }
   ],
   "source": [
    "#read the clean data using pandas\n",
    "dem = pd.read_csv(\"SUMMARYMEASURESOFHEALTH.csv\")\n",
    "\n",
    "tmp_full_X = pd.read_csv(\"communities-crime-full.csv\")\n",
    "tmp_full_X['communityname'] = tmp_full_X['communityname'].str.replace(r'(city|borough|City|township|village|town)$','')\n",
    "tmp_full_X['communityname'] = tmp_full_X['communityname'].str.replace(r'City$','')\n",
    "new_full_X = tmp_full_X.merge(dem, left_on = ['state','communityname'],right_on = ['State_FIPS_Code','CHSI_County_Name'],how='inner',suffixes=('_original', '_addition'))\n",
    "print(\"if we using the inner join, we can find the smaple number become:\", len(new_full_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, we using the left outer join, hope the few new attr can make some help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1994"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_full_X = tmp_full_X.merge(dem, left_on = ['state','communityname'],right_on = ['State_FIPS_Code','CHSI_County_Name'],how='left',suffixes=('_original', '_addition'))\n",
    "new_full_X = new_full_X.replace('?', np.nan)\n",
    "len(new_full_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after the combination, we need to get the new label_y and remove some un-related attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_reg_y = new_full_X[\"ViolentCrimesPerPop\"]\n",
    "#deal with it to get the clf y\n",
    "new_clf_y = new_reg_y.copy()\n",
    "for index, item in enumerate(new_clf_y):\n",
    "    if (item > 0.1):\n",
    "        new_clf_y[index] = 1\n",
    "    else:\n",
    "        new_clf_y[index] = 0\n",
    "len(list(new_full_X.columns[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now remove the unrelated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del new_full_X[\"state\"]\n",
    "del new_full_X[\"county\"]\n",
    "del new_full_X[\"community\"]\n",
    "del new_full_X[\"communityname\"]\n",
    "del new_full_X[\"fold\"]\n",
    "del new_full_X[\"ViolentCrimesPerPop\"]\n",
    "del new_full_X[\"County_FIPS_Code\"]\n",
    "del new_full_X[\"CHSI_State_Name\"]\n",
    "del new_full_X[\"CHSI_State_Abbr\"]\n",
    "del new_full_X[\"Strata_ID_Number\"]\n",
    "del new_full_X[\"CHSI_County_Name\"]\n",
    "del new_full_X[\"State_FIPS_Code\"]\n",
    "del new_full_X[\"US_ALE\"]\n",
    "del new_full_X[\"US_All_Death\"]\n",
    "del new_full_X[\"US_Health_Status\"]\n",
    "del new_full_X[\"US_Unhealthy_Days\"]\n",
    "len(list(new_full_X.columns[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_full_features = list(new_full_X.columns[:])\n",
    "new_full_X = imp.fit_transform(new_full_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii.Give accuracy, precision, and recall results with and without the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10-fold cv is:\n",
      "[ 0.78109453  0.755       0.725       0.75879397  0.73366834  0.7638191\n",
      "  0.81407035  0.75879397  0.73366834  0.73366834]\n",
      "the precision for 10-fold cv is:\n",
      "[ 0.84677419  0.796875    0.79508197  0.79844961  0.80172414  0.80508475\n",
      "  0.828125    0.84070796  0.792       0.79365079]\n",
      "the recall for 10-fold cv is:\n",
      "[ 0.83333333  0.8         0.784       0.848       0.768       0.784       0.816\n",
      "  0.792       0.752       0.784     ]\n",
      "the avg accuracy is: 0.755757693942\n",
      "the avg precision is: 0.809847341511\n",
      "the avg recall is: 0.796133333333\n"
     ]
    }
   ],
   "source": [
    "#we using the dt to deal with this data\n",
    "#init the DT clf\n",
    "new_dt_clf = DecisionTreeClassifier()\n",
    "new_ten_fold_accuracy = cross_val_score(new_dt_clf, new_full_X, new_clf_y, cv=10,scoring='accuracy')\n",
    "new_ten_fold_precision = cross_val_score(new_dt_clf, new_full_X, new_clf_y, cv=10,scoring='precision')\n",
    "new_ten_fold_recall = cross_val_score(new_dt_clf, new_full_X, new_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10-fold cv is:\")\n",
    "print(new_ten_fold_accuracy)\n",
    "print(\"the precision for 10-fold cv is:\")\n",
    "print(new_ten_fold_precision)\n",
    "print(\"the recall for 10-fold cv is:\")\n",
    "print(new_ten_fold_recall)\n",
    "print(\"the avg accuracy is:\",sum(new_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(new_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(new_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10-fold cv is:\n",
      "[ 0.78606965  0.755       0.735       0.75376884  0.72864322  0.74371859\n",
      "  0.8241206   0.75376884  0.73869347  0.73366834]\n",
      "the precision for 10-fold cv is:\n",
      "[ 0.85833333  0.8015873   0.76859504  0.82170543  0.79831933  0.8245614\n",
      "  0.848       0.8245614   0.79661017  0.76923077]\n",
      "the recall for 10-fold cv is:\n",
      "[ 0.81746032  0.832       0.768       0.792       0.776       0.76        0.816\n",
      "  0.792       0.752       0.8       ]\n",
      "the avg accuracy is: 0.755245156129\n",
      "the avg precision is: 0.811150417607\n",
      "the avg recall is: 0.790546031746\n"
     ]
    }
   ],
   "source": [
    "#if without data\n",
    "full_ten_fold_accuracy = cross_val_score(full_dt_clf, full_X, full_clf_y, cv=10,scoring='accuracy')\n",
    "full_ten_fold_precision = cross_val_score(full_dt_clf, full_X, full_clf_y, cv=10,scoring='precision')\n",
    "full_ten_fold_recall = cross_val_score(full_dt_clf, full_X, full_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10-fold cv is:\")\n",
    "print(full_ten_fold_accuracy)\n",
    "print(\"the precision for 10-fold cv is:\")\n",
    "print(full_ten_fold_precision)\n",
    "print(\"the recall for 10-fold cv is:\")\n",
    "print(full_ten_fold_recall)\n",
    "print(\"the avg accuracy is:\",sum(full_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(full_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(full_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can find using the new data have not give a better result so far, maybe because the less of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii.Does the added data help? What features help? Does it matter what learning method you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above result we can find the data we added is not make so much help, only a little increase in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PctKids2Par 0.362863323984\n",
      "racePctWhite 0.0916962793266\n",
      "racePctHisp 0.0505171773405\n",
      "PctLess9thGrade 0.0219382610508\n",
      "PctEmplManu 0.0160142955179\n",
      "MedRent 0.0159258528369\n",
      "PctVacMore6Mos 0.0149708445158\n",
      "PctSameState85 0.0145711031075\n",
      "PctImmigRec10 0.0143954680319\n",
      "PctHousOccup 0.0142232095289\n",
      "HousVacant 0.013055380175\n"
     ]
    }
   ],
   "source": [
    "#check the main feature\n",
    "new_dt_clf.fit(new_full_X, new_clf_y)\n",
    "new_main_feature = dict(zip(new_full_features,new_dt_clf.feature_importances_))\n",
    "for index,w in enumerate(sorted(new_main_feature, key=new_main_feature.get, reverse=True)):\n",
    "    print(w ,new_main_feature[w])\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can find that the important features did not changed a lot, all the features are from the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's change the method to GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10 cv is:\n",
      "[ 0.73134328  0.74        0.695       0.68844221  0.65829146  0.67839196\n",
      "  0.72361809  0.71859296  0.69849246  0.63819095]\n",
      "the precision for 10 cv is:\n",
      "[ 0.93902439  0.92941176  0.94444444  0.92        0.88        0.87654321\n",
      "  0.97297297  0.93670886  0.91139241  0.89552239]\n",
      "the recall for 10 cv is:\n",
      "[ 0.61111111  0.632       0.544       0.552       0.528       0.568       0.576\n",
      "  0.592       0.576       0.48      ]\n",
      "the avg accuracy is: 0.697036338408\n",
      "the avg precision is: 0.920602043613\n",
      "the avg recall is: 0.565911111111\n"
     ]
    }
   ],
   "source": [
    "#init the gnb clf with data\n",
    "new_nb_clf = GaussianNB()\n",
    "new_nb_clf.fit(new_full_X, new_clf_y)\n",
    "new_nb_clf_accuracy = cross_val_score(new_nb_clf, new_full_X, new_clf_y, cv=10,scoring='accuracy')\n",
    "new_nb_clf_precision = cross_val_score(new_nb_clf, new_full_X, new_clf_y, cv=10,scoring='precision')\n",
    "new_nb_clf_recall = cross_val_score(new_nb_clf, new_full_X, new_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10 cv is:\")\n",
    "print(new_nb_clf_accuracy)\n",
    "print(\"the precision for 10 cv is:\")\n",
    "print(new_nb_clf_precision)\n",
    "print(\"the recall for 10 cv is:\")\n",
    "print(new_nb_clf_recall)\n",
    "print(\"the avg accuracy is:\",sum(new_nb_clf_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(new_nb_clf_precision)/10)\n",
    "print(\"the avg recall is:\",sum(new_nb_clf_recall)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for 10 cv is:\n",
      "[ 0.73134328  0.73        0.705       0.71356784  0.67839196  0.69346734\n",
      "  0.75376884  0.72361809  0.71859296  0.65829146]\n",
      "the precision for 10 cv is:\n",
      "[ 0.95        0.90804598  0.94594595  0.93589744  0.90666667  0.91025641\n",
      "  0.975       0.94871795  0.91566265  0.90140845]\n",
      "the recall for 10 cv is:\n",
      "[ 0.6031746  0.632      0.56       0.584      0.544      0.568      0.624\n",
      "  0.592      0.608      0.512    ]\n",
      "the avg accuracy is: 0.710604177604\n",
      "the avg precision is: 0.92976014858\n",
      "the avg recall is: 0.582717460317\n"
     ]
    }
   ],
   "source": [
    "#init the gnb clf without data\n",
    "old_nb_clf = GaussianNB()\n",
    "old_nb_clf.fit(full_X, full_clf_y)\n",
    "old_nb_clf_accuracy = cross_val_score(old_nb_clf, full_X, full_clf_y, cv=10,scoring='accuracy')\n",
    "old_nb_clf_precision = cross_val_score(old_nb_clf, full_X, full_clf_y, cv=10,scoring='precision')\n",
    "old_nb_clf_recall = cross_val_score(old_nb_clf, full_X, full_clf_y, cv=10,scoring='recall')\n",
    "print(\"the accuracy for 10 cv is:\")\n",
    "print(old_nb_clf_accuracy)\n",
    "print(\"the precision for 10 cv is:\")\n",
    "print(old_nb_clf_precision)\n",
    "print(\"the recall for 10 cv is:\")\n",
    "print(old_nb_clf_recall)\n",
    "print(\"the avg accuracy is:\",sum(old_nb_clf_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(old_nb_clf_precision)/10)\n",
    "print(\"the avg recall is:\",sum(old_nb_clf_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can find with the data we have a better result.\n",
    "there are two reasons to get this result:\n",
    "1.our combine data number is too less that cannot influence the result.\n",
    "2.the new attr that we import from the data.org are all unrelated attr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can test this by using the inner join to the data, then see what happens, we only use dt this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if we using the inner join, we can find the smaple number become: 162\n"
     ]
    }
   ],
   "source": [
    "little_new_full_X = tmp_full_X.merge(dem, left_on = ['state','communityname'],right_on = ['State_FIPS_Code','CHSI_County_Name'],how='inner',suffixes=('_original', '_addition'))\n",
    "print(\"if we using the inner join, we can find the smaple number become:\", len(little_new_full_X))\n",
    "little_new_full_X = little_new_full_X.replace('?', np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "little_new_reg_y = little_new_full_X[\"ViolentCrimesPerPop\"]\n",
    "#deal with it to get the clf y\n",
    "little_new_clf_y = little_new_reg_y.copy()\n",
    "for index, item in enumerate(little_new_clf_y):\n",
    "    if (item > 0.1):\n",
    "        little_new_clf_y[index] = 1\n",
    "    else:\n",
    "        little_new_clf_y[index] = 0\n",
    "del little_new_full_X[\"state\"]\n",
    "del little_new_full_X[\"county\"]\n",
    "del little_new_full_X[\"community\"]\n",
    "del little_new_full_X[\"communityname\"]\n",
    "del little_new_full_X[\"fold\"]\n",
    "del little_new_full_X[\"ViolentCrimesPerPop\"]\n",
    "del little_new_full_X[\"County_FIPS_Code\"]\n",
    "del little_new_full_X[\"CHSI_State_Name\"]\n",
    "del little_new_full_X[\"CHSI_State_Abbr\"]\n",
    "del little_new_full_X[\"Strata_ID_Number\"]\n",
    "del little_new_full_X[\"CHSI_County_Name\"]\n",
    "del little_new_full_X[\"State_FIPS_Code\"]\n",
    "del little_new_full_X[\"US_ALE\"]\n",
    "del little_new_full_X[\"US_All_Death\"]\n",
    "del little_new_full_X[\"US_Health_Status\"]\n",
    "del little_new_full_X[\"US_Unhealthy_Days\"]\n",
    "another_new_full_X = little_new_full_X.copy()\n",
    "#another is the one without new data\n",
    "del another_new_full_X[\"ALE\"]\n",
    "del another_new_full_X[\"Min_ALE\"]\n",
    "del another_new_full_X[\"Max_ALE\"]\n",
    "del another_new_full_X[\"All_Death\"]\n",
    "del another_new_full_X[\"Min_All_Death\"]\n",
    "del another_new_full_X[\"Max_All_Death\"]\n",
    "del another_new_full_X[\"CI_Min_All_Death\"]\n",
    "del another_new_full_X[\"CI_Max_All_Death\"]\n",
    "del another_new_full_X[\"Health_Status\"]\n",
    "del another_new_full_X[\"Min_Health_Status\"]\n",
    "del another_new_full_X[\"Max_Health_Status\"]\n",
    "del another_new_full_X[\"CI_Min_Health_Status\"]\n",
    "del another_new_full_X[\"CI_Max_Health_Status\"]\n",
    "del another_new_full_X[\"Unhealthy_Days\"]\n",
    "del another_new_full_X[\"Min_Unhealthy_Days\"]\n",
    "del another_new_full_X[\"Max_Unhealthy_Days\"]\n",
    "del another_new_full_X[\"CI_Min_Unhealthy_Days\"]\n",
    "del another_new_full_X[\"CI_Max_Unhealthy_Days\"]\n",
    "another_new_full_features = list(another_new_full_X.columns[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "little_new_full_X = imp.fit_transform(little_new_full_X)\n",
    "another_new_full_X = imp.fit_transform(another_new_full_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg accuracy is: 0.753014705882\n",
      "the avg precision is: 0.827144522145\n",
      "the avg recall is: 0.834090909091\n"
     ]
    }
   ],
   "source": [
    "#using dt now with new feature\n",
    "little_new_dt_clf = DecisionTreeClassifier()\n",
    "little_new_ten_fold_accuracy = cross_val_score(little_new_dt_clf, little_new_full_X, little_new_clf_y, cv=10,scoring='accuracy')\n",
    "little_new_ten_fold_precision = cross_val_score(little_new_dt_clf, little_new_full_X, little_new_clf_y, cv=10,scoring='precision')\n",
    "little_new_ten_fold_recall = cross_val_score(little_new_dt_clf, little_new_full_X, little_new_clf_y, cv=10,scoring='recall')\n",
    "print(\"the avg accuracy is:\",sum(little_new_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(little_new_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(little_new_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the avg accuracy is: 0.758529411765\n",
      "the avg precision is: 0.813268398268\n",
      "the avg recall is: 0.868181818182\n"
     ]
    }
   ],
   "source": [
    "#using dt now without new feature\n",
    "another_new_dt_clf = DecisionTreeClassifier()\n",
    "another_new_ten_fold_accuracy = cross_val_score(another_new_dt_clf, another_new_full_X, little_new_clf_y, cv=10,scoring='accuracy')\n",
    "another_new_ten_fold_precision = cross_val_score(another_new_dt_clf, another_new_full_X, little_new_clf_y, cv=10,scoring='precision')\n",
    "another_new_ten_fold_recall = cross_val_score(another_new_dt_clf, another_new_full_X, little_new_clf_y, cv=10,scoring='recall')\n",
    "print(\"the avg accuracy is:\",sum(another_new_ten_fold_accuracy)/10)\n",
    "print(\"the avg precision is:\",sum(another_new_ten_fold_precision)/10)\n",
    "print(\"the avg recall is:\",sum(another_new_ten_fold_recall)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by the above analysis we can find add the new feature only increase the precision a little, so it means that the crime rate is not related to the health states in that county, which we added to predict the crime rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
